------------------------------------------------------------------------
The list of most significant changes made over time in Dragon.

Dragon 0.3.0.0 (20190402)
DRAGON_VERSION == 3000

Changes (w.r.t. Dragon 0.2.2.13):

Preview Features:

- New V.M framework: ONNX

  We have extended the exporting and importing(Runtime) for ONNX.

- Operators Refactor:

  * <NDArray> Faster implementation for following multiple axes operators:

      ``Crop``, ``Pad``, ``Tile``, ``Reduce`` and ``Transpose``.

  * <Norm> Faster implementation for fused norm operators:

      ``BatchNorm``, ``GroupNorm``, ``LayerNorm``, ``InstanceNorm``.

- Use ``Eigen`` as the default cpu math library instead of ``OpenBLAS``.

- Use ``PyBind11`` as the default python module exporter.

- Integer data types support for common operators,

  see the documentation for more detail information.

- A new workspace-local dummy name interface has been introduced,

  which unifies the naming of static and dynamic computation graph.

- The behavior of accumulating gradients have been canceled.

- Python module now has been assigned to take charge of ``Workspace``.


Bugs fixed:

- Repair the basic TensorFlow API, following the master branch.

- More reliable shape inference for static computation graph.

------------------------------------------------------------------------

Dragon 0.2.2.13 (20181204)
DRAGON_VERSION == 2213

Changes (w.r.t. Dragon 0.2.2.12):

Preview Features:

- Dismantled op kernels.

- Added FP16 support for ``NNResizeOp``, ``ProposalOp``,

  ``ROIPoolingOp``, ``ROIAlignOp``, (R-CNN components).

- Added ``DepthwiseConv2dOp``.

- [PyTorch] Added ``nn.DepthwiseConv2d``

- [PyCaffe] Added ``DepthwiseConvolutionLayer``.


Bugs fixed:

- Fixed the cuda issue that incorrect results in ``AsTypeOp``

  under ``float32 -> float16``.

- Removed the support for group convolution implemented with im2col-nhwc.

- Changed the default ``NHWC`` pack format of filter from

  ``[filter_height, filter_width, in_channels, out_channels]``(TensorFlow) to

  ``[out_channels, filter_height, filter_width, in_channels]``(CuDNN).

- Changed the VC Runtime from ``MD`` to ``MT``.

------------------------------------------------------------------------

Dragon 0.2.2.12 (20181120)
DRAGON_VERSION == 2212

Changes (w.r.t. Dragon 0.2.2.11):

Preview Features:

- Added Cambricon's CNML context.

- Added the support for Int8(Char) Tensor.

- Removed the cuda device id query from pointer.

- Added ``DropBlock2dOp``

- Added ``MaximumOp``, ``MinimumOp``, ``NLLLossOp``.

- Added CuDNN support for ``BiasAddOp``.

- Optimized memory usage of ``DropoutOp``.

- Replaced ``thread_local`` with platform TLS solution.

- Changed the default norm eps from 1e-3 to 1e-5,

  affected: ``BatchNorm``, ``BatchRenorm``, ``GroupNorm``, ``InstanceNorm``, ``L2Norm``.

- Enforced CUDA FP16 support (i.e. Removed ``WITH_CUDA_FP16``).

- [PyTorch] Added ``torch.one_hot``.

- [PyTorch] Added ``torch.log``, ``Tensor.log``, ``torch.exp`` and ``Tensor.exp``.

- [PyTorch] Added ``torch.minimum``, ``torch.maximum``,

   ``torch.clamp``, ``Tensor.clamp``, ``Tensor.clamp_``.

- [PyTorch] Added ``nn.ELU`` and ``nn.SELU``.

- [PyTorch] Added ``nn.GroupNorm``.

- [PyTorch] Added ``nn.NLLLoss``, ``nn.BCEWithLogitsLoss``,

   ``nn.L1Loss``, ``nn.MSELoss``, ``nn.SmoothL1Loss``.

- [PyTorch] Added ``nn.DropBlock2d``.

- [PyTorch] Added ``train`` and ``eval`` mode for Module,

   affected: ``nn.BatchNorm``, ``nn.Dropout``.

- [PyTorch] Deprecated the ``size_average`` and ``reduce`` in

    ``nn.Loss``, added ``reduction`` instead.

- [PyTorch] ``torch.save`` can save both ``torch.Tensor`` and other pickle values.

- [PyCaffe] Added ``DropBlockLayer``.


Bugs fixed:

- Fixed the uncomputed output in ``BiasAddGradientOp``.

- Fixed the incorrect gradients of ``ClipGradientOp``.

- Fixed the wrong results of ``math::Inv`` under ``CPUContext``.

- Fixed the issue that the default device is used on initializing NCCL.

- Removed the strictly shape check in ``SmoothL1Op``.

- Fixed wrong CXX API exporting under Win32.

- [PyTorch] Fixed an issue that multiple ``GradientGather`` are triggered by one Operator.

- [PyTorch] Fixed the schema check by in-place fundamental ops.

- [PyTorch] Fixed the missing shape and dtype after ``Tensor.copy_``.

- [PyTorch] Fixed an issue that ``Tensor.fill_`` and ``Tensor.zero_``

  will change the data type of an non-empty Tensor.

- [PyTorch] Fixed the Python2 Int(s) check.

------------------------------------------------------------------------